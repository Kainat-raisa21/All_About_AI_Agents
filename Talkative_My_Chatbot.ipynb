{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kLY8sqU3tWv",
        "outputId": "dd502b9b-5a6c-4eb4-cd41-44e94ac7ff0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m808.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.6/396.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.8/289.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.2/207.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.1/249.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.5/559.5 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install langchain -qq\n",
        "%pip install langchain_community -qq\n",
        "%pip install langchain-google-genai -qq\n",
        "%pip install python-dotenv -qq\n",
        "%pip install streamlit -qq\n",
        "%pip install langchain_experimental -qq\n",
        "%pip install sentence-transformers -qq\n",
        "%pip install langchain_chroma -qq\n",
        "%pip install langchainhub -qq\n",
        "%pip install pypdf -qq\n",
        "%pip install rapidocr-onnxruntime -qq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "_8ob7GMS33Vz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "g_api_key = os.getenv(\"GOOGLE_API_KEY\")"
      ],
      "metadata": {
        "id": "80D6nvru33Yg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrating RAG with Chatbot (full application code)"
      ],
      "metadata": {
        "id": "jZtUfgmxdurx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "dE-vvMIDeRaL",
        "outputId": "afc7a3bb-bbfd-4ef1-ee3e-005900da5764"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-af73c0d4-e453-4e8c-8323-a23bca43243d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-af73c0d4-e453-4e8c-8323-a23bca43243d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Mind's Eye of LLMs-VoT Paper.pdf to Mind's Eye of LLMs-VoT Paper.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"Mind's Eye of LLMs-VoT Paper.pdf\")\n",
        "data = loader.load()  # entire PDF is loaded as a single Document\n",
        "data"
      ],
      "metadata": {
        "id": "TcTHjVvi6g8j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9dc4206-2384-4ca0-8a63-4c20f3c132d8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 0}, page_content='Mind’s Eye of LLMs: Visualization-of-Thought Elicits\\nSpatial Reasoning in Large Language Models\\nWenshan Wu Shaoguang Mao Yadong Zhang Yan Xia Li Dong Lei Cui Furu Wei\\nMicrosoft Research\\nhttps://aka.ms/GeneralAI\\nAbstract\\nLarge language models (LLMs) have exhibited impressive performance in language\\ncomprehension and various reasoning tasks. However, their abilities in spatial\\nreasoning, a crucial aspect of human cognition, remain relatively unexplored.\\nHuman possess a remarkable ability to create mental images of unseen objects and\\nactions through a process known as the Mind’s Eye , enabling the imagination of\\nthe unseen world. Inspired by this cognitive capacity, we propose Visualization-\\nof-Thought ( VoT) prompting. V oT aims to elicit spatial reasoning of LLMs by\\nvisualizing their reasoning traces, thereby guiding subsequent reasoning steps. We\\nemployed V oT for multi-hop spatial reasoning tasks, including natural language\\nnavigation, visual navigation, and visual tiling in 2D grid worlds. Experimental\\nresults demonstrated that V oT significantly enhances the spatial reasoning abilities\\nof LLMs. Notably, V oT outperformed existing multimodal large language models\\n(MLLMs) in these tasks. While V oT works surprisingly well on LLMs, the ability\\nto generate mental images to facilitate spatial reasoning resembles the mind’s eye\\nprocess, suggesting its potential viability in MLLMs.\\nMind’s Eye of LLMs Mind’s Eye of Humans\\nInput OutputConventional Prompting\\nInput Thought …Output ThoughtChain -of-Thought\\nInput Thought\\nVisualizationThought\\nVisualization… Output\\nBehind youisthemarket . \\nTurn right tothecemetery . \\nTurn lefttothelibrary . \\nGostraight tothepost office . \\n… \\nVerbal\\nVisual…\\nMind’s Eye\\nMental Images\\nmarketlibrarypost office garage\\nChemist’s cinema\\ncemetery\\n Mind’s Eye\\nMental Images\\n…\\nVisualization -of-ThoughtText\\nFigure 1: Humans can enhance their spatial awareness and inform decisions by creating mental\\nimages during the spatial reasoning process. Similarly, large language models (LLMs) can create\\ninternal mental images . We propose the V oT prompting to elicit the \"mind’s eye\" of LLMs for spatial\\nreasoning by visualizing their thoughts at each intermediate step.arXiv:2404.03622v2  [cs.CL]  24 May 2024'),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 1}, page_content='1 Introduction\\nRecently, large language models (LLMs) [ BCE+23,BMR+20,TLI+23,JSM+23] have achieved\\nremarkable performance on various language-related tasks. However, despite their success in math\\nreasoning [ KGR+23], common sense reasoning [ LKH+22], and other reasoning tasks such as\\nsymbolic reasoning or logic reasoning [ KGR+23], their abilities in spatial reasoning still remain\\nunderexplored [RFD+21, YBL+23, MHV+24].\\nSpatial reasoning is an essential function of human cognition, allowing us to interact with the envi-\\nronment. It facilitates tasks that require understanding and reasoning about the spatial relationships\\nbetween objects and their motions. The spatial reasoning of language models largely relies on\\nlanguage to reason about spatial information, whereas human cognitive capabilities extend far beyond\\nverbal reasoning. Humans can not only create task-relevant abstract representations from visual\\nperception [ BK18 ,KC22 ], but also imagine unseen scenes through their mind’s eye . It remains\\na research topic called mental image [ She78 ] in domains of neuroscience, philosophy of mind,\\nand cognitive science. Building upon this cognitive function, humans facilitate spatial reasoning\\nby mental image manipulation, such as navigation [ Tol48 ], mental rotation [ SM71 ], mental paper\\nfolding [ SF72 ], and mental simulation [ MK09 ]. Figure 1 illustrates the human process involved in\\na navigation task. Humans enhance their spatial awareness and inform their decisions by creating\\nmental images of a route, utilizing various sensory inputs such as navigation instructions or a map\\nimage. Subsequently, they simulate route planning through the mind’s eye.\\nInspired by this cognitive mechanism, we conjecture that LLMs possess the ability to create and\\nmanipulate mental images in the mind’s eye for spatial reasoning. As illustrated in Figure 1, LLMs\\ncould potentially process and understand spatial information in various formats. They might be\\ncapable of visualizing internal states and manipulating these mental images through their mind’s eye ,\\nthereby guiding subsequent reasoning steps to enhance spatial reasoning. Therefore, we propose\\ntheVisualization-of-Thought (VoT) prompting to elicit this ability. This method augments LLMs\\nwith a visuospatial sketchpad [ Bad92 ] to visualize their reasoning steps and inform subsequent steps.\\nV oT adopts zero-shot prompting instead of relying on few-shot demonstrations or text-to-image\\nvisualization with CLIP [ RKH+21]. This choice stems from LLMs’ ability to acquire various mental\\nimages from text-based visual art [SB14, SMM21, Reg19].\\nTo evaluate the effectiveness of VoT in spatial reasoning, we selected three tasks that require spatial\\nawareness in LLMs, including natural-language navigation [ YBL+23], visual navigation, and visual\\ntiling. These tasks require an understanding of space, direction, and geometric shape reasoning. To\\nemulate human-like multisensory perception, we designed 2D grid worlds using special characters\\nas enriched input formats for the LLMs in visual navigation and visual tiling tasks. We compared\\ndifferent models (GPT-4, GPT-4V) and prompting techniques across these three tasks. The findings\\nreveal that the V oT prompting proposed in this paper consistently induces LLMs to visualize their\\nreasoning steps and inform subsequent steps. Consequently, this approach achieved significant\\nperformance improvements on the corresponding tasks.\\nThe main contributions of this paper include:\\n1. We shed light on LLMs’ mental image for spatial reasoning from a cognitive perspective,\\nconducting quantitative and qualitative analyses on the mind’s eye of LLMs and its limitations. We\\nalso explore cues about the origin of this generalized ability from code pre-training.\\n2. We develop two tasks of \"visual navigation\" and \"visual tiling\", along with corresponding\\nsynthetic datasets, emulating various sensory inputs for LLMs. These tasks are structured to support\\nvarying levels of difficulty, offering a well-designed testbed for the research on spatial reasoning.\\n3. We propose Visualization-of-Thought (VoT) prompting to elicit the mind’s eye of LLMs\\nfor spatial reasoning and provide empirical evaluations on three tasks. Experiment results prove the\\neffectiveness of V oT prompting compared with other prompting methods and existing MLLMs. This\\nability to generate mental images to facilitate spatial reasoning resembles the mind’s eye process,\\nsuggesting its potential viability in MLLMs.\\n2'),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 2}, page_content='2 Spatial Reasoning\\nSpatial reasoning refers to the ability to comprehend and reason about the spatial relationships among\\nobjects, their movements, and interactions with the environment. This skill is vital for a wide range of\\nreal-world applications such as navigation, robotics, and autonomous driving. These fields necessitate\\naction planning based on visual perception and a concrete understanding of spatial dimensions.\\nAlthough several tasks and datasets [ WBC+15,SZL22 ,MK22 ,LB18 ,RAB+20] have been devel-\\noped to probe the spatial semantics embedded in text, existing research efforts often focus on how\\nspatial terms are linguistically structured. Recently, significant achievements and impressive results\\nhave been achieved in these benchmarks by converting spatial terms to logical forms through LLMs\\nand adopting logic programming [ YIL23 ]. This implies that excelling in these tasks does not nec-\\nessarily equate to a genuine understanding of spatial information by LLMs, nor does it provide an\\naccurate measure of their spatial awareness.\\nSpatial awareness involves understanding spatial relationships, directions, distances, and geometric\\nshapes, all of which are essential for action planning in the physical world. To evaluate the spatial\\nawareness and spatial reasoning abilities of LLMs, we have selected tasks that test navigation and\\ngeometric reasoning skills, including natural language navigation, visual navigation and visual tiling.\\n2.1 Natural Language Navigation\\nNatural language navigation task [ YBL+23] was inspired by prior research on human cogni-\\ntion [GDB17] presenting participants with sequential transitions sampled from a graph structure.\\nIn this context, a square map is defined by a sequence of random walk instructions and associated\\nobjects at each location, denoted as W={(l1, o1),(l2, o2), . . . , (ln, on)}. Given a square map W,\\nand sequence of navigation instructions I={i1, . . . , i k}, the task for the model is to identify the\\nassociated object o∈Wat the specified location lwhich is determined by the navigation instructions,\\nas detailed in Equation 1 and exemplified in Appendix B.2.\\no∼p(o∈W|W={(l1, o1),(l2, o2), . . . , (ln, on)}, I) (1)\\n2.2 Visual Navigation\\nVisual navigation task presents a synthetic 2D grid world to LLM, challenging it to navigate using\\nvisual cues. The model must generate navigation instructions to move in four directions (left, right,\\nup, down) to reach the destination from the starting point while avoiding obstacles. This involves two\\nsub-tasks: route planning andnext step prediction , requiring multi-hop spatial reasoning, while the\\nformer is more complex. Task instructions are available in Figure 6 in appendix.\\nFormulation The model is presented with a grid map Mconsisting of kconsecutive edges\\nE={e(s0, s1), e(s1, s2),···, e(sk−1, sk)}, where the starting point and destination are s0andsk\\nrespectively, as shown in Figure 2. Route planning task is to generate a sequence of correct directions\\nD={d(s0, s1), d(s1, s2),···, d(sk−1, sk)}, as defined in Equation 2. Given Mandtnavigation\\ninstructions Dt,0<t<k ={d(s0, s1),···, d(st−1, st)}, next step prediction task is to identify the\\ncorrect direction d(st, st+1)of the next step, as defined in Equation 3.\\nD∼p({d(s0, s1), d(s1, s2),···, d(sk−1, sk)} |M) (2)\\n(a)k=2\\n (b)k=3\\n (c)k=4\\n (d)k=5\\n (e)k=6\\n (f)k=7\\nFigure 2: Examples of a navigation map under different settings of k, with emoji of house indicating\\nthe starting point, and emoji of office indicating the destination.\\n3'),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 3}, page_content='d∼p(d(st, st+1)|M, D t,0<t<k) (3)\\nImplementation The navigation map’s underlying graph is semi-Eulerian, alternating between\\nhorizontal and vertical edges, with 2k+1possible spatial configurations for a k-hop navigation map.\\nFor each map and set of knavigation instructions, k−1question-and-answer (QA) instances,i.e.\\n\"what is the next step?\" are created. Further implementation details are in Appendix A.1.\\n2.3 Visual Tiling\\nIntroduced by [ Gol66 ], polyomino tiling is a classic spatial reasoning challenge. We extend this\\nconcept to test the LLM’s ability to comprehend, organize, and reason with shapes in a confined area,\\nthus enhancing the evaluation of spatial reasoning skills. As depicted in Figure 3, the task involves a\\nrectangle with unfilled cells and various polyomino pieces, like the I-tetromino made of four aligned\\nsquares. The model must select the appropriate polyomino variant, such as choosing the orientation\\nfor the I-tetromino, to solve the QA puzzle. Task instructions are provided in Figure 7 in appendix.\\nFormulation The model is presented with a rectangle Rmasked with kunique polyominoes\\nMP={mp1,···, mp k}, 2 corresponding variants of each polyomino vi<=k={vi1, vi2}, and a\\npolyomino query q∈MP. Visual tiling task is to identify the correct variant of q, as defined in\\nEquation 4.\\nv∼p(vq|R,{mp1,···, mp k},{v11, v12···, vk1, vk2}, q) (4)\\nImplementation The dataset comprises valid spatial arrangements generated through existing\\nalgorithms[ ES03 ,GN07 ], with random masking of polyominoes to create QA puzzles. Details are\\nprovided in Appendix A.2.\\n(a) Fit 2 pieces into a masked rectangle\\n (b) Fit 3 pieces into a masked rectangle\\nFigure 3: Example of visual tiling with masked polyomino pieces. Variants of those polyomino\\npieces including rotation and reflection are not shown in this figure.\\n3 Visualization-of-Thought Prompting\\nConsidering the way humans process spatial information during tasks like navigation, it’s common\\nto create mental images , such as maps, to enhance spatial awareness or simulating movements to\\ninform decision-making. Our objective is to elicit the spatial awareness of LLMs and ground their\\nreasoning by visualizing the consequence of their intermediate reasoning steps.\\nWe introduce Visualization-of-Thought (VoT) prompting: \"Visualize the state after each rea-\\nsoning step.\" This new paradigm for spatial reasoning aims to generate reasoning traces and\\nvisualizations in an interleaved manner. Qualitative results of this approach are presented in Figure 4.\\nWe use pθto denote a pre-trained LM with parameters θ,x, y, z to denote a language sequence, and\\nvto denote a visualization sequence in text form. In a multi-hop spatial reasoning task with input x,\\nCoT prompting generates a series of intermediate steps z1,···, zn, each step zi∼pθ(zi|x, z 1···i−1)\\nis sampled sequentially, followed by the output y∼pθ(y|x, z 1···n). As shown in Figure 1, VoT\\nprompting enhances this process by adding a visuospatial sketchpad to each thought zi, then the\\nsubsequent thought zi+1is sampled conditioned on prior thoughts z1···iand visualizations v1···i.\\nAs defined in the Equation 5 and 6, it forms interleaved reasoning traces and visualizations. A\\nqualitative comparison between outputs of V oT and CoT is provided in Figure 8a in appendix.\\nvi∼pθ(vi|prompt V oT, x, z 1···i, v1···i−1) (5)\\n4'),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 4}, page_content=\"Starting from      , provide the steps to navigate \\nto       .Provided: I        T        L\\nTo fit all the provided polyominoes into the \\nempty squares, what's the correct variation of \\nTetromino  T?\\nVisualize the state after each reasoning step.Visual Navigation Visual Tiling Natural Language Navigation\\nYou have been given a 3 by 3 square grid. Initially, \\nyou are at the bottom -left corner…find a cassette \\nplayer…go right…a wool, go right…a conch, go \\nup…a moving van, go left…a confectionery store, \\ngo left…a pot pie, go up…a siamang, go right…a \\nblack -and-white colobus, go right…a minivan. \\nNow you have all the information on the map. You \\nstart at where the cassette player is located, then \\nyou go right by one step, go right…go up…go \\nleft…go left…go up…go right…go down by one \\nstep. What will you find?\\nVisualize the state after each reasoning step. Visualize the state after each reasoning step.\\n1. Place I 2. Place L 3. Place T\\n1. Move right 2. Move down 3. Move left\\n4. Move down 5. Move left 6. Move downAnalyze I Analyze L Analyze T\\n…\\nFigure 4: Examples of V oT prompting in three tasks, where LLM generates 2D grids as mental\\nimages . The generated reasoning traces and visualizations form an interleaved sequence to track the\\nstate over time. Full responses could be found in Appendix B.\\nzi+1∼pθ(zi+1|prompt V oT, x, z 1···i, v1···i) (6)\\nThis reasoning paradigm enables LLMs with visual state tracking. We introduce the concept of a\\nstate , denoted as si=[x, z 1···i, v1···i−1]representing a partial solution at step iwith the input, the\\nsequence of thoughts z1···iand the sequence of visualizations v1···i−1.\\nvi∼pθ(vi|prompt V oT, x, z 1···i, v1···i−1)\\n∼pθ(vi|prompt V oT, si)(7)\\nAs shown in Equation 7, visual state tracking is implemented by generating the visualization vias\\namental image of the internal state siafter each reasoning step zi(e.g.vicould be a grid of the\\nnavigation map marked with path or a filled rectangle). Grounded by the visual state tracking sequence,\\nthe subsequent state is derived by si+1∼pθ(si+1|prompt V oT, x, s i, vi). This mechanism allows\\nfor the derivation of subsequent states, reflecting spatiotemporal causality and enhancing the spatial\\nreasoning capabilities of LLMs in a grounded context.\\n4 Experiment\\n4.1 Setup\\nFor the visual tasks where a counterpart image exists for each text input, we conduct additional exper-\\niments with a multimodal model. Specifically, we adopt GPT-4 [ OA+23] and GPT-4 Vision [ Ope23 ]\\nvia Azure OpenAI API as they’re state of the art LLM and multimodal model respectively. API\\nsettings are temperature 0 as greedy decoding and top p 1, with model versions of 1106-preview and\\nvision-preview. For all experiments we adopt zero-shot prompting.\\nDepending on whether the LLM is explicitly prompted to visualize intermediate steps, we experiment\\nwith three settings of GPT-4, including zero-shot CoT prompting( GPT-4 CoT ),GPT-4 w/o Viz where\\nvisualization is explicitly disabled during reasoning, and V oT prompting ( GPT-4 VoT ). Additional\\nsetting of GPT-4 Vision with counterpart image input is GPT-4V CoT . Prompts are as following:\\n•GPT-4 CoT : Let’s think step by step.\\n•GPT-4 w/o Viz : Don’t use visualization. Let’s think step by step.\\n5\"),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 5}, page_content='•GPT-4V CoT : Let’s think step by step.\\n•GPT-4 VoT : Visualize the state after each reasoning step.\\nTask instructions and examples could be found in Appendix B.\\n4.2 Dataset\\nNatural Language Navigation We generate 200 square maps of size 3x3 which is described by 9\\nlandmarks in snake order traversal, and a set of navigation instructions.\\nVisual Navigation We generate 496 navigation maps and 2520 QA instances in total, the distribution\\nof which is provided in Table 4 in appendix.\\nVisual Tiling We first generate multiple unique configurations to fill a 5 x 4 rectangle with 5\\npolyomino pieces including two I tetrominoes, two T tetrominoes and one L tetromino. Then we\\nrandomly masked two or three pieces of different types and generate QA instance for each masked\\npieces. The total number of QA instances is 796, and we show dataset details in Table 5 in appendix.\\n4.3 Metric\\nWe extract the answer from model output by pattern matching. For tasks except for route planning,\\nwe calculate accuracy by Equation 8. We adopted sub-string matching as fcorrect to determine\\ncorrectness.\\nacc=nX\\nifcorrect (extracted _answer, ground _truth )/n (8)\\nFor the route planning task which predicts a sequence of navigation instructions, we reject any\\nsequences exceeding 100 instructions, considering them to be random guesses. We then normalize\\nthe navigation instructions by executing each navigation instruction. Those instructions which violate\\nnavigation rules will be ignored. The length tof normalized instruction sequence is considered as the\\ntemporal distance against the starting point. Given the ground-truth of knavigation instructions, the\\ncompleting rate of route planning is t/k. For the dataset of nmaps, we report two metrics including:\\n1.Average completing rate:Pn\\niti/ki/n. Average completing rate among all instruction\\nsequences, reflecting LLM’s effectiveness of route planning.\\n2.Success rate:Pn\\ni(ti==ki)/n. This metric represents the proportion of instruction se-\\nquences with t=k, i.e., reaching the destination.\\n4.4 Results\\nAs illustrated in Table 1, GPT-4 VoT significantly outperforms other settings in all tasks across\\nall metrics. The significant gap when comparing GPT-4 V oT with GPT-4V CoT and GPT-4 w/o\\nViz demonstrates that effectiveness of visual state tracking, which allows LLMs visually interpret\\ntheir actions within an grounded world. And in the natural language navigation task, GPT-4 VoT\\noutperforms GPT-4 w/o Viz by 23.5%. In the visual tasks, the noticeable performance gap between\\nGPT-4 CoT andGPT-4V CoT indicates that LLM grounded with 2D grid could possibly outperform\\na MLLM in challenging spatial reasoning tasks.\\nOn the other hand, performance of GPT-4 V oT is still far from perfect in all tasks, especially in the\\nmost challenging route planning task. Despite these tasks are relatively easy for humans, performance\\nof LLMs drops significantly as task difficulty increases.\\n5 Analysis\\nAs explained in section 3, one of the core aspects of V oT lies in enabling LLMs with visual state\\ntracking. During the experiments, it was observed that GPT-4 CoT occasionally exhibited this reason-\\ning pattern across several tasks with exception of route planning. Besides, incorrect visualizations of\\nV oT are commonly observed in model outputs. In this section, our analysis of V oT primarily focuses\\n6'),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 6}, page_content='SettingsVisual Navigation\\nVisual TilingNatural-Language\\nNavigation Route Planning Next Step\\nPredictionCompleting Rate Succ Rate\\nGPT-4 CoT 37.02 9.48 48.61 54.15 54.00\\nGPT-4 w/o Viz 37.17 10.28 48.49 46.98 35.50\\nGPT-4V CoT 33.36 5.65 46.59 49.62 /\\nGPT-4 V oT 40.77 14.72 55.28 63.94 59.00\\nTable 1: Performance of different GPT-4/4V settings in all tasks. Underline denotes statistical\\nsignificance with p < 0.05 when comparing GPT-4 V oT against all baselines using two-sample z-test,\\nwhile p < 0.16 is observed compared with GPT-4 CoT in natural language navigation task.\\non three questions: (1) Do visual state tracking behaviors differ among prompting methods? (2) How\\nvisualizations enhance final answers? (3) Can V oT benefit less powerful language models?\\n5.1 Do visual state tracking behaviors differ among prompting methods?\\nFor each model output, we extract the sequence of visualizations sampled prior to generating the final\\nanswer and discard any visualizations generated thereafter. Then we compare the sequence length lv\\nwith the number of reasoning steps ls. We calculate Complete TrackingPn\\ni(lv==ls)/nwhen a\\nvisualization vicorresponds to each state si. Similarly, we calculate the Partial Tracking metric\\nasPn\\ni(lv>0)/nwhen at least one visualization is present before the final answer is generated.\\nFigure 5 shows the significant differences between these settings. In the GPT-4 CoT setting, it\\ndemonstrated noticeable tracking rate across almost all tasks except route planning. This observation\\nimplies that LLMs inherently exhibit the capability of visual state tracking when spatiotemporal\\nsimulation is integral to reasoning .\\nOn the other hand, the visual state tracking behavior is sensitive to prompts to varying degrees. As\\nshowcased in Figure 8 in appendix, after removing \"reasoning\" from the prompt of V oT, the visualiza-\\ntions are sampled after GPT-4 generates the wrong answer. Consequently, explicitly prompting LLMs\\nto visualize their reasoning traces with VoT markedly improves the visual tracking rate , thereby\\nenhancing overall performance. The potential contribution of code pre-training to this emergent\\ncapability is further explored in Appendix C.\\nRoute\\nPlanningNext Step\\nPredictionVisual\\nTilingNatural\\nlanguage\\nNavigation05010096.292.987.1\\n20\\n1.259.3 57.4\\n0.5 0518.1\\n0%GPT-4 V oT GPT-4 CoT GPT-4 w/o Viz\\n(a) Complete tracking rateRoute\\nPlanningNext Step\\nPredictionVisual\\nTilingNatural\\nlanguage\\nNavigation05010099.295.3\\n87.480.5\\n1.891\\n59.2\\n30.5\\n05.518.3\\n0%GPT-4 V oT GPT-4 CoT GPT-4 w/o Viz\\n(b) Partial tracking rate\\nFigure 5: tracking rate of different settings across all tasksk.\\n5.2 How visualizations enhance final answers?\\nIdeally, VoT is supposed to generate an accurate visualization viat each step, so that subsequent step\\nzi+1could be determined correctly. This relies on the spatial visualization and spatial understanding\\ncapability of LLMs. To evaluate these capabilities of LLMs in these tasks, we extract the final\\nvisualization from each model output under the setting GPT-4 VoT in visual navigation and polyomino\\ntiling task. Specifically, for visual navigation task, we extract the visualized map where LLM\\ncompleted all navigation instructions. For polyomino tiling, we extract the rectangle filled with\\ncorresponding polyomino piece. The spatial visualization capability is measured by two criteria:\\n(1)Compliance , indicating whether the manipulation of mental image satisfies requirements such\\nas avoiding overlap and navigating around obstacles. (2) Accuracy , indicating whether the mental\\n7'),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 7}, page_content='image aligns with the corresponding state. The spatial understanding capability is measured by the\\nproportion of correct answers when the corresponding visualization is generated accurately.\\nAs could be seen from Table 2, the suboptimal accuracy of state visualization suggests that significant\\nenhancements to LLMs are required in the future. Despite the gap between visualization accuracy\\nand verbal reasoning performance, LLM demonstrates promising capabilities in multi-hop\\nvisualization that adhere to spatial constraints. . On the other hand, the accuracy of spatial\\nunderstanding (above 65%) is relatively high, ensuring LLM make a correct decision given accurate\\nvisualization of the internal state, which improves the groundedness and contributes to the significant\\nperformance gain. Several case studies are provided in Appendix E for interested readers.\\nTaskSpatial Visualization Spatial Understanding\\nCompliance Accuracy Accuracy\\nVisual Navigation 51.14 26.48 65.16\\nVisual Tilling 52.01 24.25 77.20\\nTable 2: Spatial visualization/understanding evaluation in visual navigation and visual tiling task.\\nOn the other hand, VoT prompting might underperform in those tasks where LLMs can leverage\\nlogical reasoning without visualizing internal states . We conducted experiments in natural language\\nnavigation within a ring [ YBL+23], where navigation instructions are either clockwise or counter-\\nclockwise movements. By normalizing each instruction to a signed number, GPT-4 converts this task\\nto mathematical calculation of adding and modulus operation. For example, instructions of 15 steps\\nclockwise and 3 steps counter-clockwise are normalized to (15 - 3) % 12. Results show that GPT-4\\nCoT outperforms GPT-4 V oT with 52.5% VS 49.5% among 200 test instances with ring size of 12.\\n5.3 Can VoT benefit less powerful language models?\\nTo evaluate the efficacy of V oT on less powerful language models, we conducted experiments across\\nvarious model families [ BMR+20,OA+23,TLI+23] and model sizes, including GPT-3.5 turbo ,\\nLLAMA3-8B-Instruct andLLAMA3-70B-Instruct . We access GPT-3.5 via Azure OpenAI API\\nwith model version 1106-preview and apply greedy decoding to all models.\\nAs shown in Table 3, within the same model family, performance improves across all tasks with\\nincreases in model size. LLAMA3-70B VoT significantly outperforms the baseline across all tasks\\nexcept for visual tiling, where it aligns closely with results observed in GPT-4. This consistency\\nsuggests that V oT offers a scaling advantage when applied to more advanced models, markedly\\nenhancing performance in larger models. In contrast, less capable models tend to rely on random\\nguessing, especially in spatial reasoning tasks. For instance, in the route planning task, GPT-3.5\\nCoT often resorts to speculative responses, random guessing in nearly half of the instances, which\\nleads to exhaustion of output tokens. While GPT-3.5 V oT effectively minimizes random guesses,\\nsuch occurrences become increasingly rare with GPT-4 CoT as the model size expands. On the\\nother hand, the reliance on random guessing introduces unpredictability in performance trends for\\nless powerful models. It suggests their limitations in sustaining reliable reasoning processes across\\ndifferent difficulty levels. Details on performance trends are provided in Appendix D.\\nSettingsVisual Navigation\\nVisual TilingNatural-Language\\nNavigation Route Planning Next Step\\nPredictionCompleting Rate Succ Rate\\nGPT-3.5 CoT 16.10 2.62 17.42 44.10 8.50\\nGPT-3.5 V oT 19.02 1.61 13.10 47.99 9.00\\nLLAMA3-8B CoT 4.65 0 28.73 47.24 16.50\\nLLAMA3-8B V oT 4.97 0.2 26.75 46.73 15.50\\nLLAMA3-70B CoT 19.90 2.62 49.01 56.41 26.00\\nLLAMA3-70B V oT 30.24 5.85 54.09 56.03 32.50\\nTable 3: Performance of V oT in GPT-3.5 and LLAMA3 models. Underline denotes statistical\\nsignificance with p < 0.05 compared to corresponding CoT baseline using two-sample z-test.\\n8'),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 8}, page_content='6 Related Works\\nSpatial Reasoning over Text Spatial reasoning and spatial language understanding [ KPM20 ] in\\nNLP domain mainly focus on semantic representation [ CBGG97 ,Bat10 ,HK11 ], spatial information\\nextraction [ RMK18 ,KVOM11 ], learning and reasoning [ KM15 ,SLYA17 ,KFP19 ]. Recent advance-\\nments have further explored spatial reasoning within the context of large language models (LLMs).\\nTo improve multi-hop spatial reasoning skills of language models, several works [ MFNK21 ,MK22 ]\\nproposed to pretrain language models with synthetic datasets. An increasing number of dataset were\\nthen developed to covers various type of spatial relations in 2D visual scenes [ WBC+15,SZL22 ],\\ngeometric patterns [ Cho19 ] and 3D spatial information [ AMKK21 ,HZC+23]. [FML+22] investi-\\ngated spatial reasoning capabilities of transformer-based models in the UI grounding setting. On the\\nother hand, some works adopted in-context learning, leveraging LLMs for general purpose reasoning\\nto convert spatial information to logic forms [ YIL23 ], or as a general pattern machine for sequence\\ntransformation [ MXF+23]. Recently, several works focused on evaluating spatial reasoning of LLMs\\nas cognitive capability on navigation [ YBL+23] and planning tasks [ MHV+24] among various spatial\\nstructures. While most existing works rely on linguistic semantics and verbal reasoning, and might\\nnot always necessitate spatial awareness, we propose to elicit mind’s eye of LLMs in spatial reasoning\\ntasks with various formats from a cognitive perspective. The V oT prompting induces LLMs to create\\nmental images for visualizing their internal states and inform subsequent reasoning step.\\nWorld Models of LLMs While there have been many theoretical debates about whether LLMs\\ncan effectively learn an internal world model from ungrounded form alone [ BHT+20,MGSS21 ],\\n[LeC22 ] advocated that world models should represent percepts and action plans at multiple levels\\nof abstraction and multiple time scales, with the capability of planning, predicting, and reasoning.\\n[LWG+22] proposed to ground LLM in the physical world by reasoning over the experimental results\\npredicted by external simulation. [ HGM+23] further leveraged LLMs as world models to predict the\\nsubsequent states by action simulation, given predefined states and actions per task. On the other\\nhand, an increasing number of studies focus on investigating internal representations of LLMs. [ PP22 ,\\nAKH+21] showed that by utilizing in-context learning, LLMs’ learned representations can be mapped\\nto grounded perceptual and conceptual structure in color and spatial domains. Moreover, [ GT23 ]\\nand [ NLW23 ] discovered linear representations of space, time and game state in specifically trained\\nLLMs, which are important for dynamic causal world models. Our work does not probe the internal\\nrepresentations of specialized LLMs, nor does it depend on external simulation engine or state\\ndefinitions. We demonstrate LLMs’ zero-shot capability of visualizing their precepts at abstract level,\\npredicting and tracking the internal states over time to generate action plans in multi-hop spatial\\nreasoning tasks, which possibly mirrors the causal world model within LLMs.\\n7 Conclusion\\nThis study introduces Visualization-of-Thought Prompting (V oT), inspired by the human cognitive\\nfunction of visualizing and manipulating mental images through the mind’s eye. We have demon-\\nstrated that V oT enables LLMs to exhibit the mechanism of \"the mind’s eye\", as evidenced by their\\nperformance in multi-hop spatial reasoning tasks and our comprehensive analysis of the reasoning\\ntraces. Remarkably, V oT enable LLMs to outperform state-of-the-art multimodal large language\\nmodels (MLLMs) in the tested visual tasks. While V oT demonstrates impressive efficacy in LLMs,\\nthis emergent capability to create mental images to enhance spatial reasoning resembles the mind’s\\neye process, suggesting its promise in MLLMs.\\nBuilding on the success of experiments with GPT-4, we plan to investigate how V oT can futher elicit\\n\"the mind’s eye\" in MLLMs to enhance their spatial awareness. Additionally, our future efforts will\\nfocus on automatic data augmentation from real-world scenarios, aiming to identify effective methods\\nfor learning generalized internal representations of mental images . This will further improve the\\nmind’s eye of LLMs, ultimately contributing to the advancement of their cognitive and reasoning\\nabilities.\\n9'),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 9}, page_content='Limitations\\nThis work only scratches the surface of spatial reasoning of LLMs. Both mental images and visual\\nstate tracking rely on the emergent ability of advanced LLMs. Therefore, it might cause performance\\ndeterioration in less advanced language models or more challenging tasks. Besides, due to the limited\\ndata exposure and a lack of explicit instruction tuning, visual state tracking of current LLMs are\\nsensitive to prompts. For example, when explicitly prompted with \"use ascii-art\", the tracking rate\\nwill significantly increase thereby boosting performance, while removing \"reasoning\" from the VoT\\nprompt will cause a decrease of tracking rate. Moreover, the mental images tested in our work are\\nlimited to 2D grid. To strength the mind’s eye of LLMs, more diverse and complicated representation\\nshould be explored in the future, such as complex geometric shapes and even 3D semantics shown in\\nFigure 11 in appendix.\\nReferences\\n[AKH+21]Mostafa Abdou, Artur Kulmizev, Daniel Hershcovich, Stella Frank, Ellie Pavlick, and\\nAnders Søgaard. Can language models encode perceptual structure without grounding?\\na case study in color. arXiv preprint arXiv:2109.06129 , 2021.\\n[AMKK21] Daich Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanqa: 3d\\nquestion answering for spatial scene understanding. 2022 IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages 19107–19117, 2021.\\n[Bad92] Alan Baddeley. Working memory. Science , 255(5044):556–559, 1992.\\n[Bat10] John A Bateman. Language and space: a two-level semantic approach based on princi-\\nples of ontological engineering. International Journal of Speech Technology , 13:29–48,\\n2010.\\n[BCE+23]Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, J. Gehrke, Eric Horvitz, Ece\\nKamar, Peter Lee, Y . Lee, Yuan-Fang Li, Scott M. Lundberg, Harsha Nori, H. Palangi,\\nMarco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early\\nexperiments with gpt-4. arXiv.org , 2023.\\n[BHT+20]Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce\\nChai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, et al.\\nExperience grounds language. arXiv preprint arXiv:2004.10151 , 2020.\\n[BK18] Nicholas Baker and Philip J Kellman. Abstract shape representation in human visual\\nperception. Journal of Experimental Psychology: General , 147(9):1295, 2018.\\n[BMR+20]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, J. Kaplan, Prafulla\\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini\\nAgarwal, Ariel Herbert-V oss, Gretchen Krueger, T. Henighan, Rewon Child, A. Ramesh,\\nDaniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric\\nSigler, Mateusz Litwin, S. Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam\\nMcCandlish, Alec Radford, I. Sutskever, and Dario Amodei. Language models are\\nfew-shot learners. Neural Information Processing Systems , 2020.\\n[CBGG97] Anthony G Cohn, Brandon Bennett, John Gooday, and Nicholas M Gotts. Representing\\nand reasoning with qualitative spatial relations about regions. In Spatial and temporal\\nreasoning , pages 97–134. Springer, 1997.\\n[Cho19] François Chollet. On the measure of intelligence, 2019.\\n[ES03] Niklas Eén and Niklas Sörensson. An extensible sat-solver. In International conference\\non theory and applications of satisfiability testing , pages 502–518. Springer, 2003.\\n[FML+22]R. Freedman, Joseph B. Mueller, Jack Ladwig, Steven Johnston, David McDonald,\\nH. Wauck, Ruta Wheelock, and Hayley Borck. A symbolic representation of human\\nposture for interpretable learning and reasoning. arXiv.org , 2022.\\n10'),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 10}, page_content='[GDB17] Mona M Garvert, Raymond J Dolan, and Timothy EJ Behrens. A map of abstract\\nrelational knowledge in the human hippocampal–entorhinal cortex. elife, 6:e17086,\\n2017.\\n[GN07] Eugene Goldberg and Yakov Novikov. Berkmin: A fast and robust sat-solver. Discrete\\nApplied Mathematics , 155(12):1549–1561, 2007.\\n[Gol66] Solomon W Golomb. Tiling with polyominoes. Journal of Combinatorial Theory ,\\n1(2):280–296, 1966.\\n[GT23] Wes Gurnee and Max Tegmark. Language models represent space and time, 2023.\\n[HGM+23]Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and\\nZhiting Hu. Reasoning with language model is planning with world model, 2023.\\n[HK11] Joana Hois and Oliver Kutz. Towards linguistically-grounded spatial logics. In Dagstuhl\\nSeminar Proceedings . Schloss Dagstuhl-Leibniz-Zentrum fÃ 1/4r Informatik, 2011.\\n[HZC+23]Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,\\nand Chuang Gan. 3d-llm: Injecting the 3d world into large language models, 2023.\\n[JSM+23]Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Deven-\\ndra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume\\nLample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock,\\nTeven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.\\nMistral 7b, 2023.\\n[KC22] Yuna Kwak and Clayton E Curtis. Unveiling the abstract format of mnemonic represen-\\ntations. Neuron , 110(11):1822–1828, 2022.\\n[KFP19] Nikhil Krishnaswamy, Scott Friedman, and James Pustejovsky. Combining deep learning\\nand qualitative spatial reasoning to learn complex structures from sparse examples with\\nnoise. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 33,\\npages 2911–2918, 2019.\\n[KGR+23]Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.\\nLarge language models are zero-shot reasoners, 2023.\\n[KM15] Parisa Kordjamshidi and Marie-Francine Moens. Global machine learning for spatial\\nontology population. Journal of Web Semantics , 30:3–21, 2015.\\n[Knu00] Donald E Knuth. Dancing links. arXiv preprint cs/0011047 , 2000.\\n[KPM20] Parisa Kordjamshidi, J. Pustejovsky, and Marie-Francine Moens. Representation, learn-\\ning and reasoning on spatial language for downstream nlp tasks. Conference on Empirical\\nMethods in Natural Language Processing , 2020.\\n[KVOM11] Parisa Kordjamshidi, Martijn Van Otterlo, and Marie-Francine Moens. Spatial role la-\\nbeling: Towards extraction of spatial relations from natural language. ACM Transactions\\non Speech and Language Processing (TSLP) , 8(3):1–36, 2011.\\n[LB18] Brenden M. Lake and Marco Baroni. Generalization without systematicity: On the\\ncompositional skills of sequence-to-sequence recurrent networks, 2018.\\n[LeC22] Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-\\n27.Open Review , 62(1), 2022.\\n[LKH+22]Xiang Lorraine Li, Adhiguna Kuncoro, Jordan Hoffmann, Cyprien de Masson d’Autume,\\nPhil Blunsom, and Aida Nematzadeh. A systematic investigation of commonsense\\nknowledge in large language models, 2022.\\n[LWG+22]Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush V osoughi, Claire Cui,\\nDenny Zhou, and Andrew M. Dai. Mind’s eye: Grounded language model reasoning\\nthrough simulation, 2022.\\n11'),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 11}, page_content='[MFNK21] Roshanak Mirzaee, Hossein Rajaby Faghihi, Qiang Ning, and Parisa Kordjmashidi.\\nSpartqa: A textual question answering benchmark for spatial reasoning. North American\\nChapter of the Association for Computational Linguistics , 2021.\\n[MGSS21] William Merrill, Yoav Goldberg, Roy Schwartz, and Noah A Smith. Provable limitations\\nof acquiring meaning from ungrounded form: What will future language models under-\\nstand? Transactions of the Association for Computational Linguistics , 9:1047–1060,\\n2021.\\n[MHV+24]Ida Momennejad, Hosein Hasanbeig, Felipe Vieira, Hiteshi Sharma, Robert Os-\\nazuwa Ness, Nebojsa Jojic, Hamid Palangi, and Jonathan Larson. Evaluating\\ncognitive maps and planning in large language models with cogeval. Arxiv:\\nhttp://arxiv.org/abs/2309.15129v1 , 2024.\\n[MK09] Samuel T Moulton and Stephen M Kosslyn. Imagining predictions: mental imagery\\nas mental emulation. Philosophical Transactions of the Royal Society B: Biological\\nSciences , 364(1521):1273–1280, 2009.\\n[MK22] Roshanak Mirzaee and Parisa Kordjamshidi. Transfer learning with synthetic corpora\\nfor spatial role labeling and reasoning. Conference on Empirical Methods in Natural\\nLanguage Processing , 2022.\\n[MXF+23]Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gon-\\nzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as\\ngeneral pattern machines, 2023.\\n[NLW23] Neel Nanda, Andrew Lee, and Martin Wattenberg. Emergent linear representations in\\nworld models of self-supervised sequence models. arXiv preprint arXiv:2309.00941 ,\\n2023.\\n[OA+23]OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,\\nFlorencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal\\nAnadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu,\\nHaiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-\\nShapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-\\nLuisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor\\nCai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael,\\nBrooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason\\nChen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cum-\\nmings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch,\\nDamien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien\\nEcoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada\\nFishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik\\nGoel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan\\nGrafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris\\nHallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris\\nHesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu,\\nShengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela\\nJiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun,\\nTomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar,\\nTabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Hendrik\\nKirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew\\nKondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael\\nLampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel\\nLim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia\\nLue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski,\\nBianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney,\\nChristine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Ja-\\ncob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan\\nMorikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin\\nNair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeon-\\nwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo,\\n12'),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 12}, page_content='Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Pas-\\nsos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,\\nMichael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass,\\nVitchyr Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri,\\nAlec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra\\nRimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted\\nSanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schul-\\nman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker,\\nPranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina\\nSlama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski\\nSuch, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine Thomp-\\nson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley,\\nJerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea\\nV oss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward,\\nJason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng,\\nMatt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren\\nWorkman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin\\nYu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang,\\nShengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4\\ntechnical report, 2023.\\n[Ope23] OpenAI. Gpt-4v(ision) system card. 2023.\\n[PP22] Roma Patel and Ellie Pavlick. Mapping language models to grounded conceptual spaces.\\nInInternational Conference on Learning Representations , 2022.\\n[RAB+20]Laura Ruis, Jacob Andreas, Marco Baroni, Diane Bouchacourt, and Brenden M Lake. A\\nbenchmark for systematic generalization in grounded language understanding. Advances\\nin neural information processing systems , 33:19861–19872, 2020.\\n[Reg19] John Regehr. Explaining code using ascii art, 2019.\\n[RFD+21]Julia Rozanova, Deborah Ferreira, Krishna Dubba, Weiwei Cheng, Dell Zhang, and\\nAndre Freitas. Grounding natural language instructions: Can large language models\\ncapture spatial information?, 2021.\\n[RKH+21]Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sand-\\nhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen\\nKrueger, and Ilya Sutskever. Learning transferable visual models from natural language\\nsupervision, 2021.\\n[RMK18] Taher Rahgooy, Umar Manzoor, and Parisa Kordjamshidi. Visually guided spatial\\nrelation extraction from text. In Proceedings of the 2018 Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, Volume 2 (Short Papers) , pages 788–794, 2018.\\n[SB14] Anthony J Smith and Joanna J Bryson. A logical approach to building dungeons: Answer\\nset programming for hierarchical procedural content generation in roguelike games. In\\nProceedings of the 50th Anniversary Convention of the AISB , 2014.\\n[SF72] Roger N Shepard and Christine Feng. A chronometric study of mental paper folding.\\nCognitive psychology , 3(2):228–243, 1972.\\n[She78] Roger N Shepard. The mental image. American psychologist , 33(2):125, 1978.\\n[SLYA17] Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. A corpus of natural language\\nfor visual reasoning. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the\\n55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short\\nPapers) , pages 217–223, Vancouver, Canada, July 2017. Association for Computational\\nLinguistics.\\n[SM71] Roger N. Shepard and Jacqueline Metzler. Mental rotation of three-dimensional objects.\\nScience , 171:701 – 703, 1971.\\n13'),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 13}, page_content='[SMM21] Harini Sampath, Alice Merrick, and Andrew Macvean. Accessibility of command line\\ninterfaces. In Proceedings of the 2021 CHI Conference on Human Factors in Computing\\nSystems , pages 1–10, 2021.\\n[SZL22] Zhengxiang Shi, Qiang Zhang, and Aldo Lipani. Stepgame: A new benchmark for\\nrobust multi-hop spatial reasoning in texts. AAAI Conference on Artificial Intelligence ,\\n2022.\\n[TLI+23]Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\\nTimothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien\\nRodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and\\nefficient foundation language models. arXiv.org , 2023.\\n[Tol48] Edward C Tolman. Cognitive maps in rats and men. Psychological review , 55(4):189,\\n1948.\\n[WBC+15]Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M. Rush, Bart van Merriënboer,\\nArmand Joulin, and Tomas Mikolov. Towards ai-complete question answering: A set of\\nprerequisite toy tasks, 2015.\\n[YBL+23]Yutaro Yamada, Yihan Bao, Andrew K. Lampinen, Jungo Kasai, and Ilker Yildirim.\\nEvaluating spatial understanding of large language models, 2023.\\n[YIL23] Zhun Yang, Adam Ishay, and Joohyung Lee. Coupling large language models with logic\\nprogramming for robust and general reasoning from text, 2023.\\nA Synthetic Data\\nA.1 Visual Navigation\\nAs depicted in 1, given a specific k, the process of generating a 2D navigation map is composed\\nof 3 steps, which are instruction generation, instruction simulation, map rendering. In instruction\\ngeneration step, we enumerate all possible instruction sets navigating from the starting point to the\\ndestination (e.g move up, then move right). During this step, only the direction of each instruction is\\ndetermined, while the moving distance is undetermined until next step. In instruction simulation step,\\nsimulation is applied in the 2D coordinate system with origin (0, 0) as the starting point. To guarantee\\nan unique answer in each navigation map, the moving distance of each instruction is dynamically\\ncalculated to avoid overlapping. Each time when an overlapping is detected, the moving distance\\nof previous instruction will be increased by 1 unit recursively until overlapping is resolved. As\\nthe distance is determined, those corresponding points are added to the navigating path. After all\\ninstructions are completed, the final point is marked as the destination. In the map rendering step, the\\nbounding box of those points is adopted and normalized to a 2D square grid. The starting point and\\ndestination are marked with dedicated squares, and cells along the path are marked by empty squares,\\nwhile other untouched cells are marked by obstacle squares.\\nSince the direction of each navigation instruction is alternating, there are 4∗2k−1= 2k+1kinds\\nof spatial configurations for a k-hop navigation map. During the implementation, we simplify the\\nrecursive implementation with an early quit when path overlapping could not be resolved within one\\niteration, the main consideration of which is the size of the map. So the number of generated map is\\nslightly lower than 2k+1as the navigating step kincreases.\\nA.2 Visual Tiling\\nThe data generation process comprises 3 stages, including configuration generation, question gen-\\neration and polyomino rendering. In the configuration generation stage, to generate valid spatial\\nconfigurations of a rectangle and the corresponding polyomino set, we convert a tiling problem to\\nexisting formalized problems. One of the problems is an exact cover problem leveraging dancing\\nlink algorithm [ Knu00 ], which could be described as: given a matrix of 0s and 1s, find a set of rows\\ncontaining exactly one 1 in each column. The conversion is to construct a matrix of 0s and 1s, each\\nrow of which represents a possible arrangement of placing a specific polyomino in a rectangle. As\\nillustrated in Equation 9, given kpolyomino pieces, and a rectangle of nunits to be filled, the first k\\n14'),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 14}, page_content='Algorithm 1: Navigation Map Generation\\nInput : k\\nOutput : Csolution ={s1, s2, ..., s n},Ctextual _map={t1, t2, ..., t n},\\nCvisual _map={v1, v2, ..., v n};where n= 2k+1\\n1dirs←[up,left,down,right]\\n2instruction _sets←gen_instruction (dirs, k )\\n3fordir_instructs ininstruction _sets do\\n4 cur_pos, path _points, moves ←initialize () // origin as the starting point\\n5 fordirection indir_instructs do\\n// instruction simulation\\n6 ifnot validate_plan(cur_pos, direction, path_points) then\\n7 increase_previous_move (cur_pos, moves, path _points )\\n8 end\\n9 cur_pos, path _points ←step_forward (cur_pos, direction, path _points )\\n10 moves ←moves ∪direction\\n11 end\\n12 si←moves\\n13 ti, vi←render_map (extract_bounding_box (path _points ))\\n14 Csolution ←Csolution ∪si\\n15 Ctextual _map←Ctextual _map∪ti\\n16 Cvisual _map←Cvisual _map∪vi\\n17end\\ncolumns compose an one-hot vector indicating the corresponding polyomino, and the last ncolumns\\nare marked with 0 or 1 depending on whether the corresponding unit is filled by that polyomino. Then\\nfinding a set of polyomino arrangements in a rectangle equals to find a set of rows containing exactly\\none 1 in each column. Another adaptable problem is the boolean satisfiability problem (commonly\\nknown as SAT), for which efficient solvers exist [ ES03 ,GN07 ]. A tiling problem can be converted to\\nSAT by introducing a boolean variable for each possible arrangement of each piece, and then adding\\nclauses comprising of those boolean variables that ensure at least one arrangement of each piece is\\nachieved, while avoiding conflicts between arrangements of one piece or two different pieces.\\nGiven the size of a rectangle and polyominoes to be fit, multiple corresponding solutions are generated\\nby applying those algorithms. Then in the question generation stage, we randomly mask several\\npolyomino pieces in the rectangle, and generate a question answer(QA) pair for each masked\\npolyomino. Finally the rectangle and each polyomino piece are rendered with emoji squares.\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0C1 CkCk+1 Cn+k\\n1 0 1 0 0 1\\n1 0 0 1 1 0\\n0 1 1 1 0 0\\n0 1 0 0 1 1P1\\nPk\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb(9)\\nA.3 Visual Data Rendering\\nAfter gathering the textual dataset of 2D square grid, we generate the corresponding visual dataset by\\ndrawing text onto an image. Specifically we adopt color emojis for a fair comparison as they’re more\\nvisual friendly to a multimodal model.\\nA.4 Dataset Details\\nData distribution among various difficulty levels for visual navigation tasks and visual tiling tasks are\\nprovided in Table 4 and 5.\\n15'),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 15}, page_content=\"TaskKStepTotal\\n2 3 4 5 6 7\\nRoute Planning 8 16 32 64 128 248 496\\nNext Step Prediction 8 32 96 256 640 1488 2520\\nTable 4: Data distribution of visual navigation dataset with\\nthe total navigating step of kindicating difficulty level. The\\nreason why the number of generated map is slightly lower\\nthan2k+1fork >5is explained in Appendix A.1.Mask countTotal\\n2 3\\nConfiguration 248 124 376\\nQA Instance 489 307 796\\nTable 5: Details of visual tiling\\ndataset. Some QA instances are\\ndiscarded when multiple solutions\\nexist and all answers are correct.\\nB Examples\\nFor visual navigation and visual tiling tasks, the structured input template is comprised of task\\ninstruction, input parameters and prompt of specific setting.\\nB.1 Visual Tasks\\nTask instructions and responses of each visual task under setting GPT-4 V oT are provided as following:\\n• Route Planning Task instruction in Figure 6, response in Figure 12 .\\n• Next Step Prediction Task instruction in Figure 6, response in Figure 13.\\n• Visual Tiling Task instruction in Figure 7, response in Figure 14.\\nNavigation  Task : for a provided  map,  \\n is the home  as starting  point,  \\n is the \\noffice  as the destination .       means  the road,  \\n means  the obstacle . There  exists  \\none and only one viable  route  for each map. Each  step you choose  a direction  and \\nmove  to the end of the continuous  road or the destination .\\nmap:\\n```\\n```\\nStarting from \\n , provide the steps to navigate to \\n .\\nVisualize the state after each reasoning step.\\n```\\nStarting from \\n , to navigate to \\n , you made following movements: \\n1. Move right to the end of continuous road.\\n2. Move down to the end of continuous road.\\n3. Move left to the end of continuous road.\\nWhat's the direction of next movement?\\nA. Up\\nB. Left\\nC. Down\\nD. Right\\nVisualize the state  after each reasoning step.Navigation Task: for a provided map, \\n  is the home as starting point, \\n  is the \\noffice as the destination.       means the road, \\n  means the obstacle. There exists \\none and only one viable route for each map. Each step you choose a direction and \\nmove to the end of the continuous road or the destination.\\nmap:\\n```Route Planning Next Step Prediction\\nFigure 6: Task Instruction of visual navigation.\\nB.2 Natural Language Navigation\\nPrompt Example You have been given a 3 by 3 square grid. Starting from a vertex, you will move\\nalong the edges of the grid. Initially, you are positioned at the bottom-left corner of the grid, where\\nyou will find a torch, then you go right, where you will find an infant bed, then you go right, where\\nyou will find an American dipper. Then you go up, where you will find a jay, then you go left, where\\nyou will find a terrapin, then you go left, where you will find a microwave oven. Then you go up,\\nwhere you will find a baseball player, then you go right, where you will find a harvestman, then you\\ngo right, where you will find a neck brace. Now you have all the information on the map. You start at\\nthe position where the torch is located, then you go right by one step, then you go right by one step,\\nthen you go up by one step, then you go up by one step, then you go left by one step, then you go\\ndown by one step, and then you go down by one step. What will you find?\\n16\"),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 16}, page_content=\"Task: given  a set of polyominoes  and corresponding  \\nvariations  of each polyomino,  fit them  into the empty  \\nsquares  (      ) in the target  rectangle  without  overlapping  any \\nexisting  polyominoes  or going  outside  the rectangle . The \\nvariations  allow  only translation,  not rotation  or reflection . \\nIt's guaranteed  that there  always  exists  a solution .\\n-------------------------\\nTarget rectangle with 12 empty squares:\\n```\\n```\\nProvided polyominoes:\\n1. Tetromino  I (       )\\n2. Tetromino  L  (       )\\n3. Tetromino  T  (       )\\n-------------------------\\nVariations for Tetromino  I:\\nVariation 1 fitting into its bounding box:\\n```Variation 2 fitting into its bounding box:\\n```\\n```\\n-------------------------\\nVariations for Tetromino  L:\\nVariation 3 fitting into its bounding box:\\n```\\n```\\nVariation 7 fitting into its bounding box:\\n```\\n```\\n-------------------------\\nVariations for Tetromino  T:\\nVariation 2 fitting into its bounding box:\\n```Variation 3 fitting into its bounding box:\\n```\\n```\\n-------------------------\\nTo fit all the provided  polyominoes  into the empty  squares  \\n(       ), what's  the correct  variation  of Tetromino  T?\\nA. 2\\nB. 3\\nC. Neither\\nVisualize  the state after each reasoning  step.\\n``````Figure 7: Task Instruction of visual tiling.\\nFirst, let's look at the target rectangle and identify the empty squares (     ) \\nwhere we need to fit the polyominoes:\\nWe have two polyominoes to fit into this rectangle: Tetromino  I (\\n ) and \\nTetromino  T (\\n ). The Tetromino  I can only fit in the bottom row since it's \\nthe only place with four consecutive empty squares, either horizontally or \\nvertically. Let's place the Tetromino  I (\\n ) in the bottom row:\\n…..\\nWe can see that Variation 2 of Tetromino  T (\\n ) fits perfectly into the \\nremaining empty squares, as it matches the T shape formed by the empty \\nsquares. Let's place Variation 2 of Tetromino  T (\\n ) into the target rectangle:\\nThe correct variation of Tetromino T to fit into the empty squares is Variation 2 .\\nAnswer: A. 2First, let's look at the empty spaces in the target rectangle. We have a 4x1 space at the \\nbottom and three single squares above it. The Tetromino  I (\\n ) can only fit in the 4x1 \\nspace at the bottom, so we'll use Variation 2 of the Tetromino  I to fill that space .\\nNow , let's consider the Tetromino  T (\\n ) variations. We have three single empty squares \\nthat are not in a straight line, so Variation 2 of the Tetromino  T, which is a straight line \\nwith a protrusion, will not fit. However, Variation 3 of the Tetromino  T has a \\nconfiguration that can fit into the three remaining empty squares with one square \\nprotruding to the side, which matches the layout of the empty squares .\\nTherefore , the correct variation of Tetromino T to fit into the remaining empty squares is :\\nB. 3\\nThe final configuration would look like this:\\nVisualize the state after each reasoning step Let’s think step by step.\\n(a) Common behaviors of V oT and CoT prompting in visual tiling task, with the overall track rate of 87.1% and\\n57.4% respectively.\\n17\"),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 17}, page_content='To navigate from \\n  to \\n , we need to find the path that avoids obstacles \\n(\\n) and follows the roads (     ). Here\\'s the step -by-step navigation :\\n1. Move down from \\n  to the end of the continuous road.\\nAfter the move:\\n2. Move left from the current position to the office \\n .\\nAfter the move:\\nThe \\n has now reached the \\n , completing the navigation task.Visualize the state after each reasoning step Visualize the state after each reasoning  step\\nTo navigate from \\n  to \\n  on the provided map, follow these instructions:\\n1. Move down to the end of the continuous road.\\n2. Move right to the end of the continuous road.\\n3. Move up to reach the destination \\n .\\nHere\\'s the visualization after each step:\\nStarting point:\\nAfter Step 1 (Move down):\\nAfter Step 2 (Move right):\\nAfter Step 3 (Move up):\\n(b) Route planning task is sensitive to prompts. After deleting the word \"reasoning\" from the V oT prompt, final\\nanswer and state is derived without conditioning on state visualization in many cases.\\nFigure 8: Example cases of visual state tracking behaviors in different tasks. Case on the left exhibits\\nvisual state tracking in an interleaved manner, where next state is conditioned on visualization of\\nprevious states. Case on the right disentangles verbal reasoning and state visualization, which leads\\nto a drop in tracking rate and task performance.\\nResponse Example See Figure 15.\\nC Visual State Tracking\\nAs for where this emergent ability stems from, it might derive from tabular data, city grid naviga-\\ntion, maze exploration related coding problems [ YBL+23]. These tasks involves understanding and\\nmanipulating objects in a 2D square grid. Besides, we conjecture the exposure of ascii-art com-\\nments [ Reg19 ] during LLMs’ code pre-training possibly enhances this generalized ability. As a fact\\nto support this conjecture, the visual tiling task is different from navigation tasks because it requires\\nshape understanding and spatial manipulation ability. While tabular data and square grid navigation\\ndata boost row-wise or column-wise attention, ascii-art supplements intricate spatial attention to\\nunderstand and manipulate 2D shapes. Additionally, ascii-art in code comments is presented in\\nvarious formats, one of which is interleaved ascii diagrams, natural language and programming\\nlanguage. It require LLMs to generate the interleaved mental images and text sequence, thereby\\nenhancing spatial visualization ability and spatiotemporal causality. Interestingly in the natural\\nlanguage navigation task, when GPT-4 is prompted with \"use ascii-art to visualize\", the complete\\ntracking rate increases to 98.5% (+78.5%), boosting task performance to 62.5% (+3.5%).\\nC.1 Ascii-art in Code Comments\\nAscii-art is commonly used in code comments to represent data structure, diagram, geometry and\\nso on, which could benefit LLMs’ spatial understanding and visualization capability. Besides, it’s\\nalso used to illustrate how an algorithm works or simulate an operation, where reasoning traces and\\ncorresponding visualization are presented in an interleaved manner. Below are several examples in\\nopen-source projects.\\n18'),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 18}, page_content='•Spatial Causality :Double-ended queue in Rust, Scrolling web pages and tree rotation\\npresent triplets of previous visual state, instruction, and updated state of instruction follow-\\ning.\\n•Temporal Causality : Undo systems from emacs provides various temporal states of the undo\\nsystem when undo operation happens in different timelines and corresponding visualizations\\nin an interleaved manner. Each visualization reflects the temporal casuality of the system\\nstate.\\nThis kind of interleaved sequence tracks the system state over time, thus reflecting spatiotemporal\\ncasuality.\\nD Performance Trends Across Levels\\nIn this analysis, we examine performance trends across varying difficulty levels in the next-step\\nprediction task for models utilizing either CoT or V oT methods. These trends are crucial for\\nunderstanding the inherent unpredictability associated with random guessing. As kincreases from\\n2 to 7 in a k-step navigation map, distinct performance patterns emerge among different models,\\nas depicted in Figure 9. Larger language models such as GPT-4 and LLAMA-70B demonstrate a\\nmore predictable decrease in accuracy with increasing k. This trend indicates a robust ability to\\nhandle progressively challenging tasks, despite the overall decrease in performance. In contrast, less\\npowerful models like GPT-3.5 and LLAMA-8B exhibit an irregular performance trajectory. These\\nmodels show variable accuracy, with significant fluctuations at higher difficulty levels, suggesting a\\nreliance on random guessing, particularly under conditions of increased task difficulty. This behavior\\nhighlights their limitations in sustaining reliable reasoning processes through more complex scenarios.\\nFurthermore, the V oT method seems to offer a modest improvement in performance for the less\\npowerful models, particularly in scenarios of lower difficulty. This observation suggests that V oT\\nmight be advantageous for enhancing reliable reasoning in simpler spatial reasoning tasks, potentially\\ncompensating for the inherent weaknesses of smaller language models.\\n2 3 4 5 6 7020406080100\\nK-step MapAccuracy (%)CoT Models Performance Trend\\n2 3 4 5 6 7020406080100\\nK-step MapAccuracy (%)V oT Models Performance Trend\\nGPT-4 GPT-3.5 LLAMA-70B LLAMA-8B\\nFigure 9: Performance Trends of CoT and V oT Models Across difficulty levels\\nE Case study\\nWe consider visual state tracking similar to spatiotemporal simulation. During the simulation in those\\ntasks, we discovered several interesting behaviors of LLM.\\n1.Diverse visualization formats for state tracking: Nearly 30 different symbols found in the\\nnavigation tasks to track the navigation progress, including marking the path, marking the current\\nlocation. Among those diverse representations, LLM succeeded in some challenging cases where it\\nused directional arrow emojis to indicate both the location and moving direction at each step. More\\nexamples could be found in Appendix E.1.\\n19'),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 19}, page_content='2.Inconsistency between language and visualization: This is commonly observed across all tasks.\\nDue to the limited visualization capability, sometimes LLM generates accurate language instruction\\nbut inaccurate visualization. And in other cases, LLM generates wrong answers even the visualization\\nis generated correctly, which reflects its limitation of spatial understanding as discussed in previous\\nsection. More examples could be found in Appendix E.2.\\n3.Self-refine mechanism: We found several cases in visual tiling tasks where spatial hallucination\\nhappens due to the inconsistency or inaccurate visualization. Subsequently, LLM refined its reasoning,\\nresulting in an accurate visualization and the correction of the final answer. More examples could be\\nfound in Appendix E.3.\\nE.1 mental images for State Tracking\\nIn the visual navigation task, LLM adopted various symbols and representations to track the state of\\nnavigation progress. As shown in Figure 10, there’re several tracking styles.\\n• Mark the path: adopting an identical symbol to mark current location or part of the path.\\n•Mark path and direction: using directional arrows to mark current location and indicate the\\nmoving direction simultaneously, which is more challenging than simply marking the path.\\n•Mark path with temporal steps: using numbers to demonstrate both temporal steps and\\ncurrent location.\\n•Remove road: turning roads into obstacles to avoid turning back, instead of adopting\\nadditional symbols to mark the path.\\nE.2 Inconsistency between Language and Visualization\\nIn the visual tiling task, two inconsistent steps are highlighted in Figure 16. One is the inconsistent\\nvisualization with the language instruction of \"place Variation 6 of Tetromino L\". Another is the\\nwrong decision to chose \"Variation 2 of Tetromino I\" given the visualization of the valid state.\\nE.3 Self-refine Mechanism\\nWe found visualization could enhance LLM’s reasoning by self-grounding and refining subsequent\\nreasoning steps in some cases. As shown in Figure 17, despite successfully identifying variation 1 of\\ntetromino L as incorrect option, GPT-4 excluded the correct option of variation 6 even it’s placed\\naccurately due to spatial hallucination (overlapping with yellow pieces), which led to a impossible\\nsolution. Then it detected the mistake and re-evaluate the placement of variation 6. Finally it placed\\nthe correct piece into the top left corner and validated the answer by filling the remaining space.\\n20'),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 20}, page_content='1. Move right Original map 2. Move down\\n3. Move right 4. Move down 5. Move leftUse arrows to reflect direction \\nOriginal map 1. Move down 2. Move right\\n3. Move down 4. Move right 5. Move downUse round pin\\nOriginal map 1. Move left 2. Move down\\n3. Move right 4. Move down 5. Move rightUse numbers for temporal steps \\nOriginal map Move downUse checklist \\nOriginal map Move rightRemove road to avoid turning back Figure 10: mental images for state tracking in visual navigation tasks.\\n21'),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 21}, page_content=\"Now,  let's unfold  the front  face (1, 2, 6, 5) downwards :  \\n```  \\n    0       3   \\n     *------- *   \\n    /       /|  \\n   *------- * |  \\n 4|       | |7  \\n   |      | *  \\n   |      |/  \\n   *------- *  \\n   |     |  \\n   |      |  \\n   *------- *  \\n  1       2  \\n```  \\nHere,  the face with vertices  1, 2, 6, and 5 has been  unfolded  downwards . The vertical  lines  connecting  the top \\nface to the front  face have  now become  horizontal,  indicating  that the front  face is lying  flat. The cube  is now \\nrepresented  as a two-dimensional  net, with the front  face unfolded . Keep  in mind  that this is a simplified  \\nrepresentation  and that in an actual  net, the faces  would  be connected  at the edges  without  the gaps shown  in the \\nASCII  art.The ASCII  art you've  provided  represents  a three -dimensional  cube  with labeled  vertices  from  0 to 7. To unfold  \\nthe front  face (which  appears  to be the face connecting  vertices  1, 2, 6, and 5), we would  lay it flat. Assuming  \\nthat the front  face is the one with vertices  1, 2, 6, and 5, and we unfold  it downwards,  the net might  look \\nsomething  like this:  \\n```  \\n    0       3   \\n     *------- *   \\n    /|     /|  \\n   *------- * |  \\n 4| |     | |7  \\n   | *----- |-*  \\n   |/     |/  \\n  *------- *  \\n 1       2  \\n```  Ascii cube:\\n```\\n```\\nUnfold the  front  face0       3 \\n     *------- * \\n  1 / |    2 /|\\n   *------- * |\\n   | |4   | |7\\n   | *-----|-* \\n   |/     | /\\n  *------- *\\n  5       6Figure 11: Representation for 3D semantics and qualitative results of GPT-4 manipulating the mental\\nimage correctly.\\nStarting from \\n , let's navigate step by step to the \\n .\\nInitial map:\\n```\\n```\\nStep 1: Move right from \\n  to the end of the continuous road.\\n```\\n```\\nStep 2: Move down to the end of the continuous road.\\n```\\n```\\nStep 3: Move left to the end of the continuous road.\\n```\\n```\\nStep 4: Move down to the end of the continuous road.\\n```\\n```\\nStep 5: Move left to the end of the continuous road.\\n```Step 6: Move down to reach the \\n .\\n```\\n```\\nThe steps to navigate from \\n  to \\n  are:\\n1. Right\\n2. Down\\n3. Left\\n4. Down\\n5. Left\\n6. Down\\nWe have now reached the destination, \\n .\\n```\\nFigure 12: Responses of Route Planning\\n22\"),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 22}, page_content=\"Let's visualize the state after each movement:\\nInitial map:\\n```\\n```\\n1. Move right to the end of the continuous road:\\n```\\n```\\n2. Move down to the end of the continuous road:\\n```\\n3. Move left to the end of the continuous road:\\n```\\n```\\nNow, the next movement options are:\\nA.Up - This would lead back to a previous position, which is not a viable route to the destination.\\nB. Left - This is not possible as there is an obstacle (\\n ) immediately to the left.\\nC. Down - This would lead to the destination (\\n ).\\nD. Right - This is not possible as there is an obstacle (\\n ) immediately to the right.\\nThe correct direction for the next movement is:\\nC. Down\\nMoving down will take us to the destination, which is the office (\\n ). Here's the final visualization:\\n```\\n```Figure 13: Response of Next Step Prediction\\n23\"),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 23}, page_content=\"Let's start by examining the empty spaces in the target rectangle and the provided \\npolyominoes and their variations. We need to fit the Tetromino I, Tetromino L, and \\nTetromino T into the empty spaces without overlapping any existing polyominoes or going \\noutside the rectangle.  \\nThe target rectangle with 12 empty squares looks like this:  \\n```  \\n \\n```  \\nWe have two variations for Tetromino I (  ), which can either be placed vertically or \\nhorizontally. Given the configuration of the empty spaces, the only way to place the \\nTetromino I is horizontally at the top row, as the vertical placement would not fit anywhere.  \\n \\nAfter placing Tetromino I (  ), the target rectangle looks like this:  \\n```  \\n \\n```  \\nNext, we have two variations for Tetromino L (  ). We can see that Variation 3 of Tetromino \\nL can fit in the bottom left corner, while Variation 7 would not fit anywhere without \\noverlapping or going outside the rectangle.  \\n \\nAfter placing Tetromino L (  ), the target rectangle looks like this:  \\n```  \\n(a) Response of Visual Tiling - Page 1\\n24\"),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 24}, page_content=' \\n```  \\nNow, we have two variations for Tetromino T (  ). We need to fit this into the remaining \\nempty spaces. Variation 2 of Tetromino T cannot fit because it would require two empty \\nspaces next to each other vertically, which we do not have. However, Variation 3 of \\nTetromino T fits perfectly into the remaini ng space.  \\n \\nAfter placing Tetromino T (  ), the target rectangle looks like this:  \\n```  \\n \\n```  \\nThe correct variation of Tetromino T that fits into the empty squares is:  \\nB. 3  \\n(b) Response of Visual Tiling - Page 2\\nFigure 14: Responses of Visual Tiling\\n25'),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 25}, page_content='(a) Response of Natural Language Navigation - Page 1\\n26'),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 26}, page_content='(b) Response of Natural Language Navigation - Page 2\\nFigure 15: Responses of Natural Language Navigation\\n27'),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 27}, page_content='(c) Response of Natural Language Navigation - Page 3\\nFigure 15: Responses of Natural Language Navigation\\n28'),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 28}, page_content=\"First, let's look at the target rectangle and identify the empty squares (   ): \\n```  \\n \\n```  \\nWe have two polyominoes to fit into this rectangle: Tetromino L (  ) and Tetromino I (  ). \\nWe need to use the variations provided to fill the empty squares without overlapping the \\nexisting polyominoes or going outside the rectangle.  \\nLet's start with Tetromino L (  ) because it has a unique shape that can fit into corners or \\nprotrusions. Looking at the variations of Tetromino L:  \\nVariation 1:  \\n```  \\n \\n```  \\nVariation 6:  \\n```  \\n \\n```  \\nWe can see that Variation 6 of Tetromino L fits perfectly into the top left corner of the target \\nrectangle, filling three empty squares vertically and turning to fill one horizontally. Let's \\nplace Variation 6 of Tetromino L  into the target rectangle:  \\n```  \\n(a) Inconsistency between language and visualization - Page 1\\n29\"),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 29}, page_content=\" \\n```  \\nNow, we have four empty squares remaining in a straight line, which matches the shape of \\nTetromino I. Let's look at the variations of Tetromino I:  \\nVariation 1:  \\n```  \\n \\n```  \\nVariation 2:  \\n```  \\n \\n```  \\nSince the remaining empty squares are in a horizontal line, Variation 2 of Tetromino I will fit \\nperfectly. Let's place Variation 2 of Tetromino I into the target rectangle:  \\n```  \\n \\n```  \\nThe correct variation of Tetromino  I that fits into the target rectangle is:  \\nB. 2  \\n(b) Inconsistency between language and visualization - Page 2\\nFigure 16: Inconsistency between language and visualization\\n30\"),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 30}, page_content=\"Let's start by visualizing the target rectangle with the empty squares (   ) and the existing \\npolyominoes:  \\n```  \\n \\n```  \\nWe have two polyominoes to fit into this rectangle: Tetromino L (  ) and Tetromino I (  ). \\nWe need to determine which variation of Tetromino L will fit without overlapping any \\nexisting polyominoes or going outside the rectangle.  \\nLet's consider the variations for Tetromino L:  \\nVariation 1:  \\n```  \\n \\n```  \\nVariation 6:  \\n```  \\n \\n```  \\nWe also have the variations for Tetromino I, but since it's a straight line, it can only fit in one \\nway in the remaining space after placing Tetromino L.  \\n \\nLet's try to fit Variation 1 of Tetromino L first:  \\n```  \\n(a) Example case of Self-refine - Page 1\\n31\"),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 31}, page_content=\" \\n```  \\nAs we can see, Variation 1 of Tetromino L does not fit because it overlaps  with the existing \\nyellow ( ) polyomino.  \\nNow let's try Variation 6 of Tetromino L:  \\n```  \\n \\n```  \\nVariation 6 of Tetromino L does not fit either because it goes outside the rectangle.  \\nSince neither Variation 1 nor Variation 6 of Tetromino L fits into the target rectangle without \\noverlapping existing polyominoes or going outside the rectangle, the correct answer is:  \\n \\nC. Neither  \\n \\nHowever, there seems to be a mistake because the task guarantees that there always \\nexists a solution . Let's re-evaluate the placement of Tetromino L.  \\nWe can see that the Tetromino L must fit in the top left corner  because that's the only place \\nwith enough space for its shape. Let's try placing Variation 1 of Tetromino L in the top left \\ncorner:  \\n```  \\n(b) Example case of Self-refine - Page 2\\nFigure 17: Example case of Self-refine\\n32\"),\n",
              " Document(metadata={'source': \"Mind's Eye of LLMs-VoT Paper.pdf\", 'page': 32}, page_content=' \\n```  \\nThis time, Variation 1 of Tetromino L fits perfectly in the top left corner.  Now, we can place \\nthe Tetromino I in the remaining space:  \\n```  \\n \\n```  \\nThe Tetromino I fits perfectly in the remaining space using Variation 1 (vertical placement). \\nTherefore, the correct variation of Tetromino L that fits into the target rectangle is:  \\n \\nA. 1  \\n \\nThe initial attempt to place Variation 1 of Tetromino L was incorrect because it was not \\nplaced in the top left corner. Upon correcting the placement, we find that Variation 1 does \\nindeed fit, and the Tetromino I can be placed afterward to complete the puz zle. \\n(c) Example case of Self-refine - Page 3\\nFigure 17: Example case of Self-refine\\n33')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = len(data)\n",
        "l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkHcbQs-eItg",
        "outputId": "4fe6a003-d742-47fd-8bca-35e8a89302b8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# split data\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
        "docs = text_splitter.split_documents(data)\n",
        "\n",
        "\n",
        "print(\"Total number of documents: \",len(docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ClqpjEmeIwY",
        "outputId": "6e542d73-9398-4cf3-d537-095db906c8d9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of documents:  104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "vector = embeddings.embed_query(\"hello, world!\")\n",
        "vector[:5]\n",
        "#vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3Xdq8i9eIzU",
        "outputId": "b9c12fc6-67dc-43fc-bcdc-f6e5670c9923"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05168594419956207,\n",
              " -0.030764883384108543,\n",
              " -0.03062233328819275,\n",
              " -0.02802734449505806,\n",
              " 0.01813092641532421]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = Chroma.from_documents(documents=docs, embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))"
      ],
      "metadata": {
        "id": "G5lvpz5C6g_b"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n"
      ],
      "metadata": {
        "id": "kYeruTMmfOZy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "chat = model.start_chat(history=[])\n",
        "\n",
        "while True:\n",
        "  prompt = input(\"Hey Human! How can I assist you?: \")\n",
        "  if (prompt.lower() == \"exit\"):\n",
        "    break\n",
        "\n",
        "  # Retrieve relevant documents\n",
        "  retrieved_docs = retriever.invoke(prompt)\n",
        "\n",
        "  # Generate response with context from retrieved documents\n",
        "  context = [doc.page_content for doc in retrieved_docs[:5]]  # Use top 5 retrieved documents\n",
        "  response = chat.send_message(prompt, stream=True)\n",
        "  for chunk in response:\n",
        "    if chunk.text:\n",
        "      print(chunk.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0RvcIugfOcR",
        "outputId": "9b90f730-38d1-460d-e97f-20b911a1b717"
      },
      "execution_count": 21,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hey Human! How can I assist you?: what is mind's eye of LLM mentioned in the paper?\n",
            "The term \"mind's eye\" of LLM is not mentioned in the\n",
            " provided paper.\n",
            "Hey Human! How can I assist you?: what is spacial reasoning in LLM?\n",
            "**Spatial reasoning in LLMs** refers to the ability of large language models (\n",
            "LLMs) to understand and reason about spatial relationships and configurations. This includes tasks such as:\n",
            "\n",
            "* **Spatial orientation:** Understanding the relative positions and orientations of\n",
            " objects in space.\n",
            "* **Spatial transformation:** Mentally rotating, translating, or otherwise transforming objects in space.\n",
            "* **Spatial visualization:** Creating mental representations of objects and their spatial relationships.\n",
            "* **Spatial planning:** Reasoning about the spatial consequences of actions or events.\n",
            "\n",
            "LLMs are trained on massive datasets of\n",
            " text and code, which includes a variety of spatial information. This allows them to learn to represent and reason about spatial concepts. For example, an LLM can be trained to:\n",
            "\n",
            "* Answer questions about the spatial relationships between objects in a scene.\n",
            "* Generate descriptions of objects and their spatial relationships.\n",
            "* Solve spatial puzzles, such as mazes or tangrams.\n",
            "* Plan paths through complex environments.\n",
            "\n",
            "Spatial reasoning is an important aspect of human intelligence, and it is essential for many tasks in the real world. LLMs are still under development, but they are showing promising results in terms of spatial reasoning. This makes them\n",
            " a valuable tool for a variety of applications, such as:\n",
            "\n",
            "* **Navigation:** LLMs can be used to help robots and self-driving cars navigate their environments.\n",
            "* **Virtual reality:** LLMs can be used to create realistic and immersive virtual environments.\n",
            "* **Design:** LLMs can be used to help designers create products and buildings that are both functional and aesthetically pleasing.\n",
            "* **Education:** LLMs can be used to help students learn about spatial concepts and solve spatial problems.\n",
            "\n",
            "As LLMs continue to improve, their ability to reason about space will become even more powerful. This will open up new possibilities for applications in a wide range of fields.\n",
            "Hey Human! How can I assist you?: what is the research paper about?\n",
            "The research paper you provided is titled \"**LLMs as Spatial Reasoners:\n",
            " A Survey**\" by Mark Alföldi, John Laird, and Thomas W. Malone. It was published in the journal AI Magazine in 202\n",
            "3.\n",
            "\n",
            "The paper surveys the state of the art in spatial reasoning in large language models (LLMs). It begins by defining spatial reasoning and discussing its importance in human intelligence and in a variety of real-world tasks.\n",
            "\n",
            "The paper then reviews the different types of spatial reasoning tasks that LLMs have been shown to\n",
            " be capable of, including:\n",
            "\n",
            "* Spatial orientation\n",
            "* Spatial transformation\n",
            "* Spatial visualization\n",
            "* Spatial planning\n",
            "\n",
            "The paper also discusses the different methods that have been used to train LLMs for spatial reasoning tasks. These methods include:\n",
            "\n",
            "* Pre-training on large datasets of text and code that contains spatial information\n",
            "* Fine-tuning on specific spatial reasoning tasks\n",
            "* Using reinforcement learning to train LLMs to solve spatial puzzles\n",
            "\n",
            "The paper concludes by discussing the challenges and opportunities for future research in spatial reasoning in LLMs. The authors argue that LLMs have the potential to become powerful tools for a variety of applications that require spatial reasoning\n",
            ", such as navigation, virtual reality, design, and education.\n",
            "\n",
            "Overall, the paper provides a comprehensive overview of the current state of the art in spatial reasoning in LLMs. It is a valuable resource for researchers and practitioners who are interested in this area.\n",
            "Hey Human! How can I assist you?: what is visualization of thought prompting?\n",
            "**Visualization of thought prompting** is a technique for generating ideas and solving problems by\n",
            " visualizing the problem space and the potential solutions. It involves:\n",
            "\n",
            "1. **Creating a mental image of the problem space.** This can be done by drawing a\n",
            " diagram, writing a description, or simply visualizing the problem in your mind.\n",
            "2. **Identifying the key elements of the problem.** These elements may include the objects involved, the relationships between the objects, and the constraints on the solution.\n",
            "3. **Visualizing potential solutions.** This can be done by mentally manipulating\n",
            " the objects in the problem space, or by sketching out different solutions on paper.\n",
            "4. **Evaluating the potential solutions.** Once you have visualized a potential solution, you can evaluate it to see if it meets the constraints of the problem and achieves the desired outcome.\n",
            "\n",
            "Visualization of thought prompting can be used to solve a wide range of problems, including:\n",
            "\n",
            "* Design problems\n",
            "* Engineering problems\n",
            "* Business problems\n",
            "* Personal problems\n",
            "\n",
            "It is a powerful technique that can help you to think more creatively and to find better solutions to problems.\n",
            "\n",
            "Here is an example of how visualization of thought prompting can be used to solve a design problem\n",
            ":\n",
            "\n",
            "**Problem:** You are designing a new product, and you need to come up with a way to make it more user-friendly.\n",
            "\n",
            "**Visualization:** You visualize the product in your mind, and you identify the key elements of the design. You notice that the product has a lot of buttons and controls, which could make it difficult for users to understand and use.\n",
            "\n",
            "**Potential solutions:** You visualize different ways to simplify the design of the product. You could reduce the number of buttons and controls, or you could rearrange them in a more logical way. You could also add labels or icons to the buttons and controls to make them easier to understand.\n",
            "\n",
            "**Evaluation:** You evaluate the potential solutions to see if they meet the constraints of the problem and achieve the desired outcome. You decide that the best solution is to reduce the number of buttons and controls, and to rearrange them in a more logical way.\n",
            "\n",
            "Visualization of thought prompting is a valuable technique that can help you to think more creatively and to find better solutions to problems. It is a simple technique that can be used by anyone, regardless of their level of experience or expertise.\n",
            "Hey Human! How can I assist you?: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zl-Uv0iLfOfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ldAkcK4AfOij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Te8Nx1cTfOlO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}